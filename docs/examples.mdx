---
title: "Examples"
description: "Runnable examples covering agent basics, tools, sessions, structured output, hooks, memory, sub-agents, and inter-agent communication."
icon: "code"
---

Detailed walkthroughs of runnable Meerkat examples. Each section explains not just *what* the code does, but *why* it is structured that way.

<Note>
All examples require an Anthropic API key exported as `ANTHROPIC_API_KEY`. Run any example from the repository root with `cargo run --example <name>`.
</Note>

## Agent basics

Core patterns for building and running agents -- from a one-liner to custom tools with budget controls.

<Accordion title="Simple chat (simple.rs)">

**Location:** `meerkat/examples/simple.rs`

The minimal "hello world" of Meerkat. Uses the fluent SDK API to send a single prompt and print the result.

```rust
#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let api_key = std::env::var("ANTHROPIC_API_KEY")
        .expect("ANTHROPIC_API_KEY environment variable must be set");

    let result = meerkat::with_anthropic(api_key)
        .model("claude-sonnet-4")
        .system_prompt("You are a helpful assistant. Be concise in your responses.")
        .max_tokens(1024)
        .run("What is the capital of France? Answer in one sentence.")
        .await?;

    println!("Response: {}", result.text);
    println!("\n--- Stats ---");
    println!("Session ID: {}", result.session_id);
    println!("Turns: {}", result.turns);
    println!("Total tokens: {}", result.usage.total_tokens());

    Ok(())
}
```

**Builder chain breakdown:**

```rust
meerkat::with_anthropic(api_key)  // Start with provider + credentials
    .model("...")                  // Required: which model
    .system_prompt("...")          // Optional: guide behavior
    .max_tokens(1024)              // Optional: limit response length
    .run("...")                    // Execute with user prompt
```

**The result struct** -- `AgentResult` gives you everything you need:

<ResponseField name="text" type="String">
  The model's response.
</ResponseField>

<ResponseField name="session_id" type="String">
  For resuming conversations later.
</ResponseField>

<ResponseField name="turns" type="u32">
  How many LLM calls were made.
</ResponseField>

<ResponseField name="usage" type="Usage">
  Token counts for cost tracking.
</ResponseField>

**Expected output:**

```text
Response: The capital of France is Paris.

--- Stats ---
Session ID: sess_abc123...
Turns: 1
Total tokens: 47
```

**Key takeaways:**
1. Meerkat handles session management, message formatting, and streaming internally.
2. The `run()` method is async because it makes network calls.
3. Sensible defaults -- you only specify what matters for your use case.

</Accordion>

<Accordion title="Custom tools (with_tools.rs)">

**Location:** `meerkat/examples/with_tools.rs`

How to give an agent access to custom tools using the full `AgentBuilder` API with explicit components.

**Architecture:**

```text
+---------------------+     +----------------------+     +-------------+
| MathToolDispatcher  |     | AnthropicLlmAdapter  |     | MemoryStore |
| (your tools)        |     | (LLM provider)       |     | (sessions)  |
+---------+-----------+     +-----------+----------+     +------+------+
          |                             |                       |
          +-----------------------------+-----------------------+
                                        |
                                +-------v-------+
                                |  AgentBuilder |
                                +-------+-------+
                                        |
                                +-------v-------+
                                |     Agent     |
                                +---------------+
```

**Step 1 -- Implement `AgentToolDispatcher`:**

```rust
struct MathToolDispatcher;

#[async_trait]
impl AgentToolDispatcher for MathToolDispatcher {
    fn tools(&self) -> Vec<ToolDef> {
        vec![
            ToolDef {
                name: "add".to_string(),
                description: "Add two numbers together".to_string(),
                input_schema: json!({
                    "type": "object",
                    "properties": {
                        "a": {"type": "number", "description": "First number"},
                        "b": {"type": "number", "description": "Second number"}
                    },
                    "required": ["a", "b"]
                }),
            },
            // ... multiply tool ...
        ]
    }

    async fn dispatch(&self, name: &str, args: &Value) -> Result<String, String> {
        match name {
            "add" => {
                let a = args["a"].as_f64().ok_or("Missing 'a' argument")?;
                let b = args["b"].as_f64().ok_or("Missing 'b' argument")?;
                Ok(format!("{}", a + b))
            }
            // ... other tools ...
            _ => Err(format!("Unknown tool: {}", name)),
        }
    }
}
```

- `tools()` returns JSON Schema definitions the LLM reads to understand available tools.
- `dispatch()` executes the tool. Return type is always `String` because results go back into the conversation.

**Step 2 -- Implement `AgentLlmClient`:**

```rust
#[async_trait]
impl AgentLlmClient for AnthropicLlmAdapter {
    async fn stream_response(
        &self,
        messages: &[Message],
        tools: &[ToolDef],
        max_tokens: u32,
        temperature: Option<f32>,
        provider_params: Option<&serde_json::Value>,
    ) -> Result<LlmStreamResult, meerkat::AgentError> {
        let request = LlmRequest {
            model: self.model.clone(),
            messages: messages.to_vec(),
            tools: tools.to_vec(),
            max_tokens,
            temperature,
            stop_sequences: None,
            provider_params: provider_params.cloned(),
        };

        let mut stream = self.client.stream(&request);
        // ... process stream events ...

        Ok(LlmStreamResult {
            content,
            tool_calls,
            stop_reason,
            usage,
        })
    }
}
```

The `AgentLlmClient` trait abstracts over LLM providers. Meerkat-core doesn't care if you use Anthropic, OpenAI, or a local model.

**Step 3 -- Build and run:**

```rust
let llm = Arc::new(AnthropicLlmAdapter::new(api_key, "claude-sonnet-4".to_string()));
let tools = Arc::new(MathToolDispatcher);
let store = Arc::new(MemoryStore::new());

let mut agent = AgentBuilder::new()
    .model("claude-sonnet-4")
    .system_prompt("You are a math assistant. Use the provided tools to perform calculations.")
    .max_tokens_per_turn(1024)
    .build(llm, tools, store);

let result = agent
    .run("What is 25 + 17, and then multiply the result by 3?".to_string())
    .await?;
```

**Expected output:**

```text
Response: Let me calculate that for you.

First, 25 + 17 = 42

Then, 42 * 3 = 126

So the final answer is 126.

--- Stats ---
Session ID: sess_xyz...
Turns: 3
Tool calls: 2
Total tokens: 234
```

<Info>
Notice that `turns: 3` even though you only called `run()` once. This is because:
1. Turn 1: LLM decides to call `add(25, 17)`
2. Turn 2: LLM receives "42", decides to call `multiply(42, 3)`
3. Turn 3: LLM receives "126", generates final response
</Info>

**Key takeaways:**
1. The agent loops automatically when the LLM wants to use a tool.
2. Tools are JSON-defined via JSON Schema.
3. Components are wrapped in `Arc` because they are shared across async tasks.
4. The full API gives you control over every component.

</Accordion>

<Accordion title="Budget limits">

You can constrain agent execution with `max_turns` to prevent runaway tool loops:

```rust
let result = meerkat::with_anthropic(api_key)
    .model("claude-sonnet-4")
    .max_tokens(1024)
    .max_turns(5)  // Stop after 5 LLM calls
    .run("Solve this complex problem step by step")
    .await?;
```

The agent will stop and return whatever it has after reaching the turn limit, even if the LLM wanted to continue.

</Accordion>

<Accordion title="Streaming events">

The agent emits `AgentEvent` values during execution. When using the SDK directly you get the final result; when using the RPC or REST surfaces, events stream to the client in real time.

<Tabs>
  <Tab title="JSON-RPC">
Events are delivered as `session/event` notifications:

```json
{"jsonrpc":"2.0","method":"session/event","params":{"session_id":"...","event":{"type":"text_delta","content":"Hello"}}}
{"jsonrpc":"2.0","method":"session/event","params":{"session_id":"...","event":{"type":"tool_use","name":"add","input":{"a":2,"b":3}}}}
{"jsonrpc":"2.0","method":"session/event","params":{"session_id":"...","event":{"type":"turn_end","usage":{"input_tokens":50,"output_tokens":20}}}}
```
  </Tab>
  <Tab title="REST">
Events stream as Server-Sent Events when using the streaming endpoint:

```bash
curl -N -H "Content-Type: application/json" \
  -d '{"prompt":"Hello","model":"claude-sonnet-4-5","stream":true}' \
  http://localhost:3000/v1/sessions
```
  </Tab>
</Tabs>

</Accordion>

## Sessions

Session lifecycle and multi-turn conversations using a single agent instance.

<Accordion title="Multi-turn with stateful tools (multi_turn_tools.rs)">

**Location:** `meerkat/examples/multi_turn_tools.rs`

Demonstrates stateful tools that persist data across conversation turns and multiple `run()` calls on the same agent.

**Stateful tool dispatcher:**

```rust
struct MultiToolDispatcher {
    state: std::sync::Mutex<AppState>,
}

struct AppState {
    notes: Vec<String>,        // Persists across turns
    calculations: Vec<f64>,    // History of calculations
}
```

The dispatcher uses `Mutex` for thread-safe access. For a single-threaded runtime you could use `RefCell`, but `Mutex` is safer.

**Tools defined:**

| Tool | Purpose |
|------|---------|
| `calculate` | Evaluate arithmetic expressions |
| `save_note` | Store a note for later |
| `get_notes` | Retrieve all saved notes |
| `get_calculation_history` | Show past calculations |

**Conversation flow:**

```rust
// Turn 1: Calculate and save
let result = agent
    .run("Calculate 15 * 8, then save a note about the result.".to_string())
    .await?;
// LLM calls calculate("15 * 8") -> "120"
// LLM calls save_note("The calculation 15 * 8 equals 120")

// Turn 2: More calculations
let result = agent
    .run("Now calculate 100 / 4 and 25 + 37.".to_string())
    .await?;
// LLM calls calculate("100 / 4") -> "25"
// LLM calls calculate("25 + 37") -> "62"

// Turn 3: Review history (proves state persists)
let result = agent
    .run("Show me all the calculations we've done and any notes we've saved.".to_string())
    .await?;
// LLM calls get_calculation_history() -> "[120.0, 25.0, 62.0]"
// LLM calls get_notes() -> "1. The calculation 15 * 8 equals 120"
```

**Key pattern -- reusing the same agent instance:**

```rust
let mut agent = AgentBuilder::new()
    // ... configuration ...
    .build(llm, tools, store);

// Reuse the SAME agent instance
let result1 = agent.run("First message".to_string()).await?;
let result2 = agent.run("Second message".to_string()).await?;  // Remembers result1
let result3 = agent.run("Third message".to_string()).await?;   // Remembers result1 + result2
```

Each `run()` call adds the user message to the session, calls the LLM with the full conversation history, adds the assistant response, and saves via the store.

**Expected output:**

```text
=== Multi-Turn Tool Usage Example ===

--- Turn 1: Calculate and save a note ---
Response: I calculated 15 * 8 = 120 and saved a note about it.
Tool calls: 2

--- Turn 2: More calculations ---
Response: Here are the results: 100 / 4 = 25, and 25 + 37 = 62.
Tool calls: 2

--- Turn 3: Review calculation history and notes ---
Response: Here's what we've done so far:

Calculations: [120.0, 25.0, 62.0]

Notes:
1. The calculation 15 * 8 equals 120

Tool calls: 2

--- Turn 4: Save a summary ---
Response: I've saved a summary note of our calculation session.
Tool calls: 1

=== Final Statistics ===
Session ID: sess_...
Total turns: 11
Total tokens: 1847
```

**Key takeaways:**
1. Tools can maintain state between turns, enabling complex workflows.
2. Conversation context is managed automatically by the agent.
3. The `turns` field shows total LLM calls across all `run()` invocations.
4. Tool state and conversation state are separate concerns.

</Accordion>

<Accordion title="Session lifecycle via JSON-RPC">

Create a session, run multiple turns, then archive it:

```json
// 1. Create session + first turn
{
  "jsonrpc": "2.0", "id": 1,
  "method": "session/create",
  "params": {
    "prompt": "You are a research assistant.",
    "model": "claude-sonnet-4-5"
  }
}

// 2. Continue with a follow-up turn
{
  "jsonrpc": "2.0", "id": 2,
  "method": "turn/start",
  "params": {
    "session_id": "01936f8a-...",
    "prompt": "Summarize what we discussed."
  }
}

// 3. Archive when done
{
  "jsonrpc": "2.0", "id": 3,
  "method": "session/archive",
  "params": { "session_id": "01936f8a-..." }
}
```

See the [JSON-RPC surface](/surfaces/rpc) for the full method reference.

</Accordion>

## Structured output

Force the agent to produce validated JSON conforming to a user-provided schema. See the full [structured output guide](/guides/structured-output) for schema compatibility details across providers.

<Accordion title="Schema extraction via REST">

<CodeGroup>

```json Request
{
  "prompt": "Analyze this code review and extract findings",
  "model": "claude-opus-4-6",
  "output_schema": {
    "schema": {
      "type": "object",
      "properties": {
        "findings": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "severity": {"type": "string", "enum": ["critical", "warning", "info"]},
              "file": {"type": "string"},
              "line": {"type": "integer"},
              "message": {"type": "string"}
            },
            "required": ["severity", "file", "message"]
          }
        },
        "summary": {"type": "string"}
      },
      "required": ["findings", "summary"]
    },
    "name": "code_review",
    "strict": false,
    "compat": "lossy",
    "format": "meerkat_v1"
  },
  "structured_output_retries": 3
}
```

```json Response
{
  "session_id": "01936f8a-...",
  "text": "{\"findings\":[{\"severity\":\"warning\",\"file\":\"main.rs\",\"line\":42,\"message\":\"Unused variable\"}],\"summary\":\"One minor issue found.\"}",
  "turns": 1,
  "tool_calls": 0,
  "usage": {"input_tokens": 120, "output_tokens": 80, "total_tokens": 200},
  "structured_output": {
    "findings": [
      {
        "severity": "warning",
        "file": "main.rs",
        "line": 42,
        "message": "Unused variable"
      }
    ],
    "summary": "One minor issue found."
  },
  "schema_warnings": null
}
```

</CodeGroup>

</Accordion>

<Accordion title="Schema extraction via JSON-RPC">

```json
{
  "jsonrpc": "2.0",
  "id": 1,
  "method": "session/create",
  "params": {
    "prompt": "Extract the key entities from: 'Apple released the iPhone 16 in Cupertino'",
    "output_schema": {
      "type": "object",
      "properties": {
        "entities": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "name": {"type": "string"},
              "type": {"type": "string"}
            },
            "required": ["name", "type"]
          }
        }
      },
      "required": ["entities"]
    },
    "structured_output_retries": 2
  }
}
```

</Accordion>

<Accordion title="Key points">

- When `output_schema` is provided, the `text` response field contains the raw JSON string; the `structured_output` field contains the parsed value.
- `structured_output_retries` (default: 2) controls how many times the agent re-attempts extraction if the LLM output fails schema validation.
- `schema_warnings` reports per-provider compatibility issues (e.g., keywords stripped for Gemini).
- `output_schema` can be provided as a raw JSON Schema or as a wrapper object with explicit `name`, `strict`, `compat`, and `format` fields.

</Accordion>

## MCP tools

Connect external MCP servers so the agent can call their tools. See the [MCP surface](/surfaces/mcp) for full configuration.

<Accordion title="Adding an MCP server via CLI">

```bash
# Stdio transport
rkat mcp add filesystem -- npx @anthropic/mcp-filesystem /path/to/dir

# HTTP/SSE transport
rkat mcp add weather --url https://weather-api.example.com/mcp

# Verify
rkat mcp list
```

Config is stored in `.rkat/mcp.toml` (project-level) or `~/.config/rkat/mcp.toml` (user-level). Once added, the agent automatically discovers tools from all configured MCP servers.

</Accordion>

<Accordion title="Using MCP tools in a session">

After adding MCP servers, enable built-in tools so the agent can access them:

<Tabs>
  <Tab title="CLI">
```bash
rkat run --enable-builtins "List the files in the project directory"
```
  </Tab>
  <Tab title="JSON-RPC">
```json
{
  "jsonrpc": "2.0", "id": 1,
  "method": "session/create",
  "params": {
    "prompt": "List the files in the project directory",
    "model": "claude-sonnet-4-5",
    "enable_builtins": true
  }
}
```
  </Tab>
</Tabs>

</Accordion>

## Hooks

Lifecycle hooks run custom logic before/after tool execution or at turn boundaries. Hooks let you implement approval workflows, audit logging, or input sanitization without modifying agent code. See the full [hooks guide](/guides/hooks) for the `HookRunOverrides` schema and config-file registration.

<Accordion title="Observer and audit logging">

A `post_tool_execution` hook that logs every tool call to Slack (fire-and-forget):

<Tabs>
  <Tab title="REST">
```json
{
  "prompt": "Delete the temp files in /tmp/scratch",
  "model": "claude-sonnet-4-5",
  "enable_builtins": true,
  "hooks_override": {
    "entries": [
      {
        "id": "audit-log",
        "point": "pre_tool_execution",
        "mode": "blocking",
        "command": ["python3", "audit_log.py"],
        "timeout_ms": 5000
      },
      {
        "id": "notify-slack",
        "point": "post_tool_execution",
        "mode": "background",
        "command": ["curl", "-X", "POST", "https://hooks.slack.com/..."],
        "timeout_ms": 10000
      }
    ],
    "disable": []
  }
}
```
  </Tab>
  <Tab title="JSON-RPC">
```json
{
  "jsonrpc": "2.0",
  "id": 1,
  "method": "session/create",
  "params": {
    "prompt": "Summarize today's logs",
    "hooks_override": {
      "entries": [
        {
          "id": "approve-shell",
          "point": "pre_tool_execution",
          "mode": "blocking",
          "command": ["./approve.sh"],
          "timeout_ms": 30000
        }
      ],
      "disable": ["default-rate-limiter"]
    }
  }
}
```
  </Tab>
</Tabs>

</Accordion>

<Accordion title="Guardrail (blocking pre-tool hook)">

A blocking `pre_tool_execution` hook that must approve every tool call before it runs. A non-zero exit code from the hook causes a `hook_denied` error (RPC code -32004).

```json
{
  "id": "approve-dangerous-tools",
  "point": "pre_tool_execution",
  "mode": "blocking",
  "command": ["./approve.sh"],
  "timeout_ms": 30000
}
```

<Warning>
Blocking hooks must complete before the operation proceeds. Use a generous `timeout_ms` for interactive approval scripts.
</Warning>

</Accordion>

<Accordion title="Key points">

- `entries` are appended after any config-file hooks. `disable` removes hooks by ID before the merged list is evaluated.
- `"background"` hooks fire-and-forget; failures are logged but do not block.
- Hook points: `pre_tool_execution`, `post_tool_execution`.

</Accordion>

## Memory and compaction

Semantic memory indexes past conversation content so the agent can recall it later. When compaction runs, older messages are summarized and indexed for retrieval via the `memory_search` tool. See the full [memory guide](/guides/memory).

<Accordion title="Long conversation with recall">

Enable memory when creating a session, then query past context in later turns:

<CodeGroup>

```json Create session with memory
{
  "jsonrpc": "2.0",
  "id": 1,
  "method": "session/create",
  "params": {
    "prompt": "You are a research assistant. Remember everything I tell you.",
    "model": "claude-sonnet-4-5",
    "enable_builtins": true,
    "enable_memory": true
  }
}
```

```json Query past context
{
  "jsonrpc": "2.0",
  "id": 2,
  "method": "turn/start",
  "params": {
    "session_id": "01936f8a-...",
    "prompt": "What did I tell you about the Rust borrow checker last week?"
  }
}
```

</CodeGroup>

The agent calls the `memory_search` tool internally to retrieve relevant indexed content from earlier compacted turns.

</Accordion>

<Accordion title="Key points">

- Requires the binary to be compiled with the `memory-store-session` feature.
- Memory is per-session: indexed content is scoped to the session's history.
- Compaction is triggered automatically based on `CompactionConfig` thresholds (token count, message count, or turn count).
- Use `capabilities/get` (RPC) or `GET /capabilities` (REST) to check if `memory_store` is available in your build.

</Accordion>

## Sub-agents

Spawn child agents for parallel work. Sub-agents inherit the parent's tool dispatcher but run with their own session and token budget. See the full [sub-agents guide](/guides/sub-agents).

<Accordion title="Parallel research via JSON-RPC">

```json
{
  "jsonrpc": "2.0",
  "id": 1,
  "method": "session/create",
  "params": {
    "prompt": "Research these three topics in parallel: Rust async, Go goroutines, Erlang processes",
    "model": "claude-opus-4-6",
    "enable_builtins": true,
    "enable_subagents": true
  }
}
```

The agent gains access to sub-agent tools (`fork`, `spawn`) and can delegate subtasks. Results are collected back into the parent session.

</Accordion>

<Accordion title="Key points">

- Requires the binary to be compiled with the `sub-agents` feature.
- Sub-agents are disabled by default to prevent recursive spawning. The factory automatically disables sub-agents and comms for child agents.
- Use `enable_subagents: true` in `session/create` params (RPC) or via `AgentBuildConfig.override_subagents` in code.
- Sub-agents share the parent's LLM client factory but get their own session store (in-memory by default).

</Accordion>

## Multi-agent comms

Two completely separate agent instances communicating over TCP using encrypted channels. See the full [comms guide](/guides/comms).

<Accordion title="Two agents exchanging messages (comms_verbose.rs)">

**Location:** `meerkat/examples/comms_verbose.rs`

**Architecture:**

```text
+----------------------------------------------------------------+
|                        AGENT A                                 |
|  +--------------+  +---------------+  +------------------+     |
|  | CommsManager |  | TCP Listener  |  | Agent + LLM      |     |
|  | (keypair A)  |  | (port 12345)  |  | (send_message    |     |
|  +--------------+  +---------------+  |  list_peers)     |     |
|                                       +------------------+     |
+----------------------------------------------------------------+
                              |
                              | TCP + Encryption
                              v
+----------------------------------------------------------------+
|                        AGENT B                                 |
|  +--------------+  +---------------+  +------------------+     |
|  | CommsManager |  | TCP Listener  |  | Agent + LLM      |     |
|  | (keypair B)  |  | (port 12346)  |  | (processes inbox)|     |
|  +--------------+  +---------------+  +------------------+     |
+----------------------------------------------------------------+
```

**Step 1 -- Identity setup (cryptographic keys):**

```rust
let keypair_a = Keypair::generate();
let keypair_b = Keypair::generate();
let pubkey_a = keypair_a.public_key();
let pubkey_b = keypair_b.public_key();
```

Each agent has a cryptographic identity. Messages are signed and encrypted, ensuring authentication, integrity, and confidentiality.

**Step 2 -- Trusted peers configuration:**

```rust
// Agent A knows about Agent B
let trusted_for_a = TrustedPeers {
    peers: vec![TrustedPeer {
        name: "agent-b".to_string(),
        pubkey: pubkey_b,
        addr: format!("tcp://{}", addr_b),
    }],
};

// Agent B knows about Agent A
let trusted_for_b = TrustedPeers {
    peers: vec![TrustedPeer {
        name: "agent-a".to_string(),
        pubkey: pubkey_a,
        addr: format!("tcp://{}", addr_a),
    }],
};
```

<Note>
Agents only accept messages from peers in their trusted list. This prevents spam and ensures security in multi-agent systems.
</Note>

**Step 3 -- CommsManager and TCP listeners:**

```rust
let config_a = CommsManagerConfig::with_keypair(keypair_a)
    .trusted_peers(trusted_for_a.clone());
let comms_manager_a = CommsManager::new(config_a);

let _handle_a = spawn_tcp_listener(
    &addr_a.to_string(),
    secret_a,
    Arc::new(trusted_for_a.clone()),
    comms_manager_a.inbox_sender().clone(),
).await?;
```

The `CommsManager` handles message encryption/decryption, inbox queue management, and outbound routing. Each agent listens on its own TCP port.

**Step 4 -- Comms tools for the LLM:**

```rust
let tools_a = CommsToolDispatcher::new(
    comms_manager_a.router().clone(),
    Arc::new(trusted_for_a),
);
```

This gives the LLM two tools: `send_message` and `list_peers`.

**Step 5 -- Build `CommsAgent`:**

```rust
let agent_a_inner = AgentBuilder::new()
    .model(&model)
    .system_prompt(
        "You are Agent A. You can communicate with other agents using the send_message tool."
    )
    .build(llm_a, tools_a, store.clone());

let mut agent_a = CommsAgent::new(agent_a_inner, comms_manager_a);
```

`CommsAgent` wraps a regular `Agent` and adds automatic inbox polling and message injection into the conversation.

**Execution flow:**

```text
Phase 1: Agent A sends message
------------------------------
User -> Agent A: "Send 'Hello from Agent A!' to agent-b"
Agent A LLM: Calls send_message tool
send_message: Encrypts + TCP sends to Agent B's port
Agent A LLM: "I've sent the message"

Phase 2: Message delivery (500ms delay)
---------------------------------------
TCP packet arrives at Agent B's listener
Listener: Verifies signature, decrypts, pushes to inbox

Phase 3: Agent B processes inbox
--------------------------------
Agent B: Checks inbox, finds message from agent-a
Agent B LLM: Receives message as context
Agent B LLM: "I received: 'Hello from Agent A!' from agent-a"
```

**Example logging output:**

```text
+==============================================================+
|  ANTHROPIC API CALL #1 - Agent A
+==============================================================+
| Model: claude-opus-4-6
| Tools provided: ["send_message", "list_peers"]
| Messages in context: 2
| Last user message: Send the message 'Hello from Agent A!' to agent-b...
+==============================================================+

+--- LLM REQUESTED TOOL: send_message ---+
| Args: {"to":"agent-b","message":"Hello from Agent A!"}
+----------------------------------------+

+==============================================================+
|  TOOL EXECUTION #1 - Agent A calling 'send_message'
+==============================================================+
| Args: {
|   "to": "agent-b",
|   "message": "Hello from Agent A!"
| }
| SUCCESS: Message sent to agent-b
+==============================================================+
```

**Key takeaways:**
1. Agents are fully independent -- each has its own keypair, port, LLM client, and state.
2. Security is built-in via cryptographic signatures and encryption.
3. The LLM uses tools to interact with the comms layer.
4. Messages arrive asynchronously; agents check when ready.

</Accordion>

## Building your own

Templates, patterns, and testing guidance for writing new examples.

<Accordion title="Example template">

```rust
//! Brief description of what this example demonstrates
//!
//! Run with:
//! ```bash
//! ANTHROPIC_API_KEY=your-key cargo run --example your_example
//! ```

use meerkat::prelude::*;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // 1. Setup: Get configuration from environment
    let api_key = std::env::var("ANTHROPIC_API_KEY")
        .expect("ANTHROPIC_API_KEY environment variable must be set");

    // 2. Build your agent (choose one approach)

    // Option A: Simple SDK (no tools)
    let result = meerkat::with_anthropic(api_key)
        .model("claude-sonnet-4")
        .run("Your prompt here")
        .await?;

    // Option B: Full builder (with tools)
    // let llm = Arc::new(YourLlmAdapter::new(api_key));
    // let tools = Arc::new(YourToolDispatcher);
    // let store = Arc::new(MemoryStore::new());
    // let mut agent = AgentBuilder::new()
    //     .model("claude-sonnet-4")
    //     .build(llm, tools, store);
    // let result = agent.run("Your prompt".to_string()).await?;

    // 3. Handle the result
    println!("Response: {}", result.text);

    Ok(())
}
```

</Accordion>

<Accordion title="Common patterns">

**Tool with external state:**

```rust
struct DatabaseTools {
    pool: Arc<sqlx::PgPool>,
}

#[async_trait]
impl AgentToolDispatcher for DatabaseTools {
    // ... tool definitions ...

    async fn dispatch(&self, name: &str, args: &Value) -> Result<String, String> {
        match name {
            "query" => {
                let sql = args["sql"].as_str().ok_or("Missing sql")?;
                // Use self.pool to execute query
                Ok("results...".to_string())
            }
            _ => Err(format!("Unknown tool: {}", name)),
        }
    }
}
```

**Composing multiple tool dispatchers:**

```rust
struct CompositeDispatcher {
    dispatchers: Vec<Arc<dyn AgentToolDispatcher>>,
}

#[async_trait]
impl AgentToolDispatcher for CompositeDispatcher {
    fn tools(&self) -> Vec<ToolDef> {
        self.dispatchers.iter()
            .flat_map(|d| d.tools())
            .collect()
    }

    async fn dispatch(&self, name: &str, args: &Value) -> Result<String, String> {
        for dispatcher in &self.dispatchers {
            if dispatcher.tools().iter().any(|t| t.name == name) {
                return dispatcher.dispatch(name, args).await;
            }
        }
        Err(format!("Unknown tool: {}", name))
    }
}
```

**Validating tool arguments:**

```rust
async fn dispatch(&self, name: &str, args: &Value) -> Result<String, String> {
    match name {
        "send_email" => {
            let to = args["to"].as_str()
                .ok_or("Missing 'to' field")?;
            let subject = args["subject"].as_str()
                .ok_or("Missing 'subject' field")?;
            let body = args["body"].as_str()
                .ok_or("Missing 'body' field")?;

            if !to.contains('@') {
                return Err("Invalid email address".to_string());
            }

            send_email(to, subject, body).await
                .map_err(|e| format!("Failed to send: {}", e))
        }
        _ => Err(format!("Unknown tool: {}", name)),
    }
}
```

</Accordion>

<Accordion title="Error handling best practices">

<CodeGroup>

```rust Good: descriptive error messages
async fn dispatch(&self, name: &str, args: &Value) -> Result<String, String> {
    let path = args["path"].as_str()
        .ok_or("Missing required 'path' argument")?;

    std::fs::read_to_string(path)
        .map_err(|e| format!("Failed to read '{}': {}", path, e))
}
```

```rust Bad: generic errors
async fn dispatch(&self, name: &str, args: &Value) -> Result<String, String> {
    let path = args["path"].as_str().ok_or("error")?;
    std::fs::read_to_string(path).map_err(|_| "failed".to_string())
}
```

</CodeGroup>

</Accordion>

<Accordion title="Testing your agent">

```rust
#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test]
    async fn test_tool_dispatch() {
        let dispatcher = MathToolDispatcher;

        // Test add
        let result = dispatcher.dispatch(
            "add",
            &json!({"a": 2, "b": 3})
        ).await;
        assert_eq!(result, Ok("5".to_string()));

        // Test missing argument
        let result = dispatcher.dispatch(
            "add",
            &json!({"a": 2})
        ).await;
        assert!(result.is_err());
    }

    #[tokio::test]
    #[ignore]  // Requires API key
    async fn test_full_agent() {
        let api_key = std::env::var("ANTHROPIC_API_KEY").unwrap();
        let result = meerkat::with_anthropic(api_key)
            .model("claude-sonnet-4")
            .run("Say 'hello' and nothing else")
            .await
            .unwrap();

        assert!(result.text.to_lowercase().contains("hello"));
    }
}
```

</Accordion>
